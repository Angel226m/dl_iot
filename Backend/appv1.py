#!/usr/bin/env python3
"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CRACKGUARD BACKEND v5.1 - DUAL MODEL SUPPORT ğŸš€
âœ… Soporte para 2 modelos: Legacy (SMP UnetPlusPlus) + Custom (UNet++ CBAM)
âœ… SelecciÃ³n de modelo vÃ­a parÃ¡metro 'model' en /api/predict
âœ… Subida de fotos con multipart/form-data (3Ã— mÃ¡s rÃ¡pido que base64)
âœ… Sin tÃºnel Cloudflare (FRP directo)
âœ… Limpieza automÃ¡tica de fotos antiguas
URL: https://crackguard.angelproyect.com
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from flask import Flask, request, jsonify, Response, send_file
from flask_cors import CORS
from werkzeug.utils import secure_filename
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2
import segmentation_models_pytorch as smp
from datetime import datetime, timedelta
import traceback
from scipy.ndimage import binary_fill_holes
from collections import Counter
from functools import lru_cache
import warnings
import time
import threading
import glob
import hashlib
import timm

warnings.filterwarnings('ignore')

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURACIÃ“N
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

app = Flask(__name__)

CORS(app, resources={
    r"/*": {
        "origins":  ["*"],
        "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
        "allow_headers": ["Content-Type", "Authorization", "Accept"],
        "expose_headers": ["Content-Type", "Content-Length", "ETag"]
    }
})

class Config:
    UPLOAD_FOLDER = '/app/uploads'
    
    # ğŸ†• DUAL MODEL PATHS
    MODEL_PATH_LEGACY = os.getenv('MODEL_PATH_LEGACY', '/app/model/best_model.pth')
    MODEL_PATH_CUSTOM = os.getenv('MODEL_PATH_CUSTOM', '/app/model/modelof.pth')
    DEFAULT_MODEL = os.getenv('DEFAULT_MODEL', 'legacy')  # 'legacy' o 'custom'
    
    MAX_CONTENT_LENGTH = 50 * 1024 * 1024
    ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'bmp', 'tiff', 'tif'}

    # Legacy model config
    LEGACY_ARCHITECTURE = 'UnetPlusPlus'
    LEGACY_ENCODER = 'timm-efficientnet-b8'
    
    # Custom model config
    CUSTOM_ENCODER = 'convnext_base.fb_in22k_ft_in1k'
    CUSTOM_USE_CBAM = True
    
    TARGET_SIZE = 640
    MEAN = [0.485, 0.456, 0.406]
    STD = [0.229, 0.224, 0.225]

    THRESHOLD = 0.5
    MIN_CRACK_COVERAGE = 0.25

    USE_TTA = True
    TTA_TRANSFORMS = ['original', 'hflip', 'vflip', 'rotate90', 'rotate180', 'rotate270']

    USE_MORPHOLOGY = True
    USE_CONNECTED_COMPONENTS = True
    MIN_COMPONENT_SIZE = 5

    OVERLAY_COLOR = 'red'
    OVERLAY_ALPHA = 0.4

    ANGLE_TOLERANCE = 12
    MIN_CRACK_LENGTH = 10

    MAX_IMAGE_DIMENSION = 2048
    MAX_GRIETAS_ANALIZAR = 10

    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    TORCH_THREADS = 4
    CV2_THREADS = 4

    PNG_COMPRESSION = 6
    JPEG_QUALITY = 92
    USE_JPEG_OUTPUT = False

    DEVICE_HEARTBEAT_TIMEOUT = 30
    DEVICE_CHECK_INTERVAL = 10
    MAX_PHOTOS_PER_DEVICE = 5
    PHOTO_RETENTION_HOURS = 24
    CLEANUP_INTERVAL = 3600

    ENABLE_ETAG = True

config = Config()
app.config['MAX_CONTENT_LENGTH'] = config.MAX_CONTENT_LENGTH
Path(config.UPLOAD_FOLDER).mkdir(exist_ok=True, parents=True)

torch.set_num_threads(config.TORCH_THREADS)
torch.set_num_interop_threads(config.TORCH_THREADS)
cv2.setNumThreads(config.CV2_THREADS)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• CUSTOM UNET++ ARCHITECTURE (CBAM)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CBAM(nn.Module):
    """Convolutional Block Attention Module"""
    def __init__(self, channels, reduction=16):
        super(CBAM, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.fc = nn.Sequential(
            nn.Conv2d(channels, channels // reduction, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // reduction, channels, 1, bias=False)
        )
        self.conv = nn.Conv2d(2, 1, 7, padding=3, bias=False)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        channel_att = self.sigmoid(avg_out + max_out)
        x = x * channel_att
        
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        spatial_att = self.sigmoid(self.conv(torch.cat([avg_out, max_out], dim=1)))
        return x * spatial_att

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, use_attention=True):
        super(ConvBlock, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        if use_attention and config.CUSTOM_USE_CBAM: 
            self.attention = CBAM(out_channels)
        else:
            self.attention = nn.Identity()

    def forward(self, x):
        return self.attention(self.conv(x))

class CustomUNetPlusPlus(nn.Module):
    """Custom UNet++ with CBAM"""
    def __init__(self, encoder_name='convnext_base.fb_in22k_ft_in1k', deep_supervision=False):
        super(CustomUNetPlusPlus, self).__init__()
        self.deep_supervision = deep_supervision

        self.stem = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )

        self.encoder = timm.create_model(
            encoder_name,
            pretrained=False,
            features_only=True,
            out_indices=(0, 1, 2, 3)
        )

        with torch.no_grad():
            dummy = torch.randn(1, 3, 640, 640)
            feats = self.encoder(dummy)
            enc_channels = [f.shape[1] for f in feats]
            del dummy, feats

        nb_filter = [64, enc_channels[0], enc_channels[1], enc_channels[2], enc_channels[3]]

        self.adapt1 = nn.Conv2d(enc_channels[0], nb_filter[1], 1)
        self.adapt2 = nn.Conv2d(enc_channels[1], nb_filter[2], 1)
        self.adapt3 = nn.Conv2d(enc_channels[2], nb_filter[3], 1)
        self.adapt4 = nn.Conv2d(enc_channels[3], nb_filter[4], 1)

        self.conv0_1 = ConvBlock(nb_filter[0] + nb_filter[1], nb_filter[0], use_attention=True)
        self.conv0_2 = ConvBlock(nb_filter[0]*2 + nb_filter[1], nb_filter[0], use_attention=True)
        self.conv0_3 = ConvBlock(nb_filter[0]*3 + nb_filter[1], nb_filter[0], use_attention=True)
        self.conv0_4 = ConvBlock(nb_filter[0]*4 + nb_filter[1], nb_filter[0], use_attention=True)

        self.conv1_1 = ConvBlock(nb_filter[1] + nb_filter[2], nb_filter[1], use_attention=False)
        self.conv1_2 = ConvBlock(nb_filter[1]*2 + nb_filter[2], nb_filter[1], use_attention=False)
        self.conv1_3 = ConvBlock(nb_filter[1]*3 + nb_filter[2], nb_filter[1], use_attention=False)

        self.conv2_1 = ConvBlock(nb_filter[2] + nb_filter[3], nb_filter[2], use_attention=False)
        self.conv2_2 = ConvBlock(nb_filter[2]*2 + nb_filter[3], nb_filter[2], use_attention=False)

        self.conv3_1 = ConvBlock(nb_filter[3] + nb_filter[4], nb_filter[3], use_attention=False)

        self.final1 = nn.Conv2d(nb_filter[0], 1, kernel_size=1)
        self.final2 = nn.Conv2d(nb_filter[0], 1, kernel_size=1)
        self.final3 = nn.Conv2d(nb_filter[0], 1, kernel_size=1)
        self.final4 = nn.Conv2d(nb_filter[0], 1, kernel_size=1)

        self.classification_head = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(enc_channels[3], 1)
        )

    def _upsample_like(self, src, target):
        return F.interpolate(src, size=target.shape[2:], mode='bilinear', align_corners=False)

    def forward(self, x):
        orig_size = x.shape[2:]

        X0_0 = self.stem(x)
        encoder_outputs = self.encoder(x)
        X1_0 = self.adapt1(encoder_outputs[0])
        X2_0 = self.adapt2(encoder_outputs[1])
        X3_0 = self.adapt3(encoder_outputs[2])
        X4_0 = self.adapt4(encoder_outputs[3])

        cls_out = self.classification_head(encoder_outputs[3])

        X0_1 = self.conv0_1(torch.cat([X0_0, self._upsample_like(X1_0, X0_0)], 1))
        X1_1 = self.conv1_1(torch.cat([X1_0, self._upsample_like(X2_0, X1_0)], 1))
        X2_1 = self.conv2_1(torch.cat([X2_0, self._upsample_like(X3_0, X2_0)], 1))
        X3_1 = self.conv3_1(torch.cat([X3_0, self._upsample_like(X4_0, X3_0)], 1))

        X0_2 = self.conv0_2(torch.cat([X0_0, X0_1, self._upsample_like(X1_1, X0_0)], 1))
        X1_2 = self.conv1_2(torch.cat([X1_0, X1_1, self._upsample_like(X2_1, X1_0)], 1))
        X2_2 = self.conv2_2(torch.cat([X2_0, X2_1, self._upsample_like(X3_1, X2_0)], 1))

        X0_3 = self.conv0_3(torch.cat([X0_0, X0_1, X0_2, self._upsample_like(X1_2, X0_0)], 1))
        X1_3 = self.conv1_3(torch.cat([X1_0, X1_1, X1_2, self._upsample_like(X2_2, X1_0)], 1))

        X0_4 = self.conv0_4(torch.cat([X0_0, X0_1, X0_2, X0_3, self._upsample_like(X1_3, X0_0)], 1))

        if self.deep_supervision:
            output1 = F.interpolate(self.final1(X0_1), size=orig_size, mode='bilinear', align_corners=False)
            output2 = F.interpolate(self.final2(X0_2), size=orig_size, mode='bilinear', align_corners=False)
            output3 = F.interpolate(self.final3(X0_3), size=orig_size, mode='bilinear', align_corners=False)
            output4 = F.interpolate(self.final4(X0_4), size=orig_size, mode='bilinear', align_corners=False)
            return [output1, output2, output3, output4], cls_out
        else: 
            output = F.interpolate(self.final4(X0_4), size=orig_size, mode='bilinear', align_corners=False)
            return output, cls_out

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VARIABLES GLOBALES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

connected_devices = {}
device_commands = {}
device_lock = threading.Lock()
latest_photos_metadata = {}

# ğŸ†• DUAL MODEL SUPPORT
model_legacy = None
model_custom = None
model_legacy_loaded = False
model_custom_loaded = False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LIMPIEZA AUTOMÃTICA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def cleanup_old_photos():
    while True:
        time.sleep(config.CLEANUP_INTERVAL)
        try:
            now = datetime.now()
            retention_time = timedelta(hours=config.PHOTO_RETENTION_HOURS)
            all_photos = glob.glob(os.path.join(config.UPLOAD_FOLDER, '*.jpg'))
            all_photos.extend(glob.glob(os.path.join(config.UPLOAD_FOLDER, '*.jpeg')))
            all_photos.extend(glob.glob(os.path.join(config.UPLOAD_FOLDER, '*.png')))

            device_photos = {}
            temp_files = []

            for photo in all_photos:
                basename = os.path.basename(photo)
                if basename.startswith('overlay_') or basename.startswith('input_'):
                    file_time = datetime.fromtimestamp(os.path.getmtime(photo))
                    temp_files.append({'path': photo, 'time': file_time})
                    continue

                parts = basename.split('_')
                if len(parts) >= 2:
                    device_id = parts[0]
                else:
                    continue

                if device_id not in device_photos:
                    device_photos[device_id] = []

                file_time = datetime.fromtimestamp(os.path.getmtime(photo))
                device_photos[device_id].append({'path': photo, 'time':  file_time})

            deleted_count = 0

            for device_id, photos in device_photos.items():
                photos.sort(key=lambda x: x['time'], reverse=True)
                for idx, photo_info in enumerate(photos):
                    photo_age = now - photo_info['time']
                    if photo_age > retention_time or idx >= config.MAX_PHOTOS_PER_DEVICE:
                        try:
                            os.remove(photo_info['path'])
                            deleted_count += 1
                            if device_id in latest_photos_metadata:
                                if latest_photos_metadata[device_id].get('filepath') == photo_info['path']: 
                                    del latest_photos_metadata[device_id]
                        except Exception as e:
                            print(f"âš ï¸ Error borrando {photo_info['path']}: {e}")

            temp_retention = timedelta(hours=1)
            for temp_file in temp_files: 
                file_age = now - temp_file['time']
                if file_age > temp_retention:
                    try:
                        os.remove(temp_file['path'])
                        deleted_count += 1
                    except Exception as e: 
                        print(f"âš ï¸ Error borrando temp {temp_file['path']}:  {e}")

            if deleted_count > 0:
                print(f"ğŸ§¹ Limpieza:  {deleted_count} archivos eliminados")

        except Exception as e:
            print(f"âŒ Error en cleanup_old_photos: {e}")

def cleanup_offline_devices():
    while True:
        time.sleep(config.DEVICE_CHECK_INTERVAL)
        with device_lock:
            now = time.time()
            offline_devices = []
            for device_id, info in list(connected_devices.items()):
                last_seen = info.get('last_seen', 0)
                if now - last_seen > config.DEVICE_HEARTBEAT_TIMEOUT:
                    offline_devices.append(device_id)
                    del connected_devices[device_id]
                    if device_id in device_commands:
                        del device_commands[device_id]
            if offline_devices:
                print(f"ğŸ§¹ Dispositivos offline: {offline_devices}")

cleanup_thread = threading.Thread(target=cleanup_offline_devices, daemon=True)
cleanup_thread.start()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# UTILIDADES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in config.ALLOWED_EXTENSIONS

def calculate_etag(filepath):
    if not config.ENABLE_ETAG: 
        return None
    try:
        with open(filepath, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        return f'"{file_hash}"'
    except: 
        return None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• CARGAR MODELOS DUAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def cargar_modelo_legacy():
    """Cargar modelo Legacy (SMP UnetPlusPlus)"""
    global model_legacy, model_legacy_loaded
    try:
        print(f"ğŸ¤– Cargando modelo LEGACY: {config.LEGACY_ARCHITECTURE} {config.LEGACY_ENCODER}...")
        if not os.path.exists(config.MODEL_PATH_LEGACY):
            print(f"   âš ï¸ Modelo no encontrado: {config.MODEL_PATH_LEGACY}")
            return False

        model_legacy = smp.UnetPlusPlus(
            encoder_name=config.LEGACY_ENCODER,
            encoder_weights=None,
            in_channels=3,
            classes=1,
            activation=None,
        )

        checkpoint = torch.load(config.MODEL_PATH_LEGACY, map_location=config.DEVICE, weights_only=False)

        if isinstance(checkpoint, dict):
            if 'swa_model_state_dict' in checkpoint: 
                state_dict = checkpoint['swa_model_state_dict']
            elif 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            elif 'ema_state_dict' in checkpoint and checkpoint['ema_state_dict']: 
                state_dict = checkpoint['ema_state_dict']
            else:
                state_dict = checkpoint
        else:
            state_dict = checkpoint

        model_legacy.load_state_dict(state_dict, strict=False)
        model_legacy = model_legacy.to(config.DEVICE)
        model_legacy.eval()

        if config.DEVICE.type == 'cpu':
            torch.set_grad_enabled(False)

        print(f"   âœ… Modelo LEGACY cargado en {config.DEVICE}")
        model_legacy_loaded = True
        return True

    except Exception as e:
        print(f"âŒ Error cargando modelo LEGACY: {e}")
        traceback.print_exc()
        return False

def cargar_modelo_custom():
    """Cargar modelo Custom (UNet++ CBAM)"""
    global model_custom, model_custom_loaded
    try:
        print(f"ğŸ¤– Cargando modelo CUSTOM: UNet++ + CBAM {config.CUSTOM_ENCODER}...")
        if not os.path.exists(config.MODEL_PATH_CUSTOM):
            print(f"   âš ï¸ Modelo no encontrado:  {config.MODEL_PATH_CUSTOM}")
            return False

        model_custom = CustomUNetPlusPlus(encoder_name=config.CUSTOM_ENCODER, deep_supervision=False)

        checkpoint = torch.load(config.MODEL_PATH_CUSTOM, map_location=config.DEVICE, weights_only=False)

        if isinstance(checkpoint, dict):
            state_dict = checkpoint.get('model_state_dict', checkpoint)
        else:
            state_dict = checkpoint

        missing, unexpected = model_custom.load_state_dict(state_dict, strict=False)
        
        if missing:
            print(f"   âš ï¸ Claves faltantes: {len(missing)}")
        if unexpected:
            print(f"   âš ï¸ Claves inesperadas:  {len(unexpected)}")

        model_custom = model_custom.to(config.DEVICE)
        model_custom.eval()

        if config.DEVICE.type == 'cpu':
            torch.set_grad_enabled(False)

        print(f"   âœ… Modelo CUSTOM cargado en {config.DEVICE}")
        model_custom_loaded = True
        return True

    except Exception as e:
        print(f"âŒ Error cargando modelo CUSTOM: {e}")
        traceback.print_exc()
        return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API REST - RASPBERRY PI (sin cambios)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.route('/api/rpi/register', methods=['POST', 'OPTIONS'])
def rpi_register():
    if request.method == 'OPTIONS': 
        return jsonify({'status': 'ok'}), 200

    try:
        data = request.get_json()
        device_id = data.get('device_id')
        device_type = data.get('type', 'raspberry_pi')
        capabilities = data.get('capabilities', [])
        ip_local = data.get('ip_local', request.remote_addr)
        stream_port = data.get('stream_port', 8889)
        tunnel_type = data.get('tunnel', 'frp')

        if not device_id: 
            return jsonify({'error': 'device_id requerido'}), 400

        with device_lock:
            connected_devices[device_id] = {
                'type': device_type,
                'capabilities': capabilities,
                'ip_local': ip_local,
                'stream_port': stream_port,
                'streaming_active': False,
                'tunnel_type': tunnel_type,
                'connected_at': datetime.now().isoformat(),
                'last_seen': time.time()
            }

            if device_id not in device_commands:
                device_commands[device_id] = []

        print(f"\n{'='*70}")
        print(f"âœ… Raspberry Pi registrado: {device_id}")
        print(f"   IP:  {ip_local}")
        print(f"   Stream port: {stream_port}")
        print(f"   TÃºnel: {tunnel_type}")
        print(f"{'='*70}\n")

        return jsonify({
            'status': 'registered',
            'device_id':  device_id,
            'backend_version': '5.1',
            'upload_method': 'multipart',
            'timestamp': datetime.now().isoformat()
        }), 200

    except Exception as e:
        print(f"âŒ Error register:  {e}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/rpi/heartbeat', methods=['POST', 'OPTIONS'])
def rpi_heartbeat():
    if request.method == 'OPTIONS': 
        return jsonify({'status':  'ok'}), 200

    try:
        data = request.get_json()
        device_id = data.get('device_id')
        streaming_status = data.get('streaming_active', False)

        if not device_id or device_id not in connected_devices: 
            return jsonify({'error':  'No registrado'}), 404

        with device_lock:
            connected_devices[device_id]['last_seen'] = time.time()
            connected_devices[device_id]['streaming_active'] = streaming_status

        commands = []
        with device_lock:
            if device_id in device_commands and device_commands[device_id]: 
                commands = device_commands[device_id].copy()
                device_commands[device_id] = []

        return jsonify({
            'status':  'ok',
            'commands': commands,
            'timestamp': time.time()
        }), 200

    except Exception as e: 
        print(f"âŒ Error heartbeat: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/rpi/photo', methods=['POST', 'OPTIONS'])
def rpi_upload_photo():
    if request.method == 'OPTIONS':
        return jsonify({'status': 'ok'}), 200

    try:
        device_id = request.form.get('device_id') or request.headers.get('X-Device-ID')

        if not device_id: 
            return jsonify({'error': 'device_id requerido'}), 400

        if device_id not in connected_devices: 
            return jsonify({'error':  'Dispositivo no registrado'}), 404

        if 'image' not in request.files:
            return jsonify({'error': 'No se enviÃ³ imagen'}), 400

        file = request.files['image']

        if not file or file.filename == '':
            return jsonify({'error': 'Nombre de archivo vacÃ­o'}), 400

        if '.' in file.filename:
            original_ext = file.filename.rsplit('.', 1)[1].lower()
        else:
            original_ext = 'jpg'

        if original_ext not in config.ALLOWED_EXTENSIONS:
            return jsonify({'error': f'Formato no permitido: {original_ext}'}), 400

        with device_lock:
            connected_devices[device_id]['last_seen'] = time.time()

        timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{device_id}_{timestamp_str}.{original_ext}"
        filepath = os.path.join(config.UPLOAD_FOLDER, filename)

        file.save(filepath)

        file_size_kb = os.path.getsize(filepath) / 1024

        metadata = {}
        if 'resolution' in request.form:
            metadata['resolution'] = request.form.get('resolution')
        if 'color_profile' in request.form:
            metadata['color_profile'] = request.form.get('color_profile')

        etag = calculate_etag(filepath)

        latest_photos_metadata[device_id] = {
            'filename': filename,
            'filepath':  filepath,
            'timestamp': datetime.now().isoformat(),
            'size_kb': round(file_size_kb, 2),
            'metadata': metadata,
            'etag': etag
        }

        print(f"\nğŸ“¸ Foto de {device_id}:  {filename} ({file_size_kb:.1f}KB) [MULTIPART]")

        return jsonify({
            'status': 'saved',
            'device_id': device_id,
            'filename': filename,
            'size_kb': round(file_size_kb, 2),
            'upload_method': 'multipart',
            'timestamp': datetime.now().isoformat()
        }), 200

    except Exception as e:
        print(f"âŒ Error upload_photo: {e}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/rpi/latest-photo/<device_id>', methods=['GET', 'OPTIONS'])
def get_latest_photo(device_id):
    if request.method == 'OPTIONS':
        return jsonify({'status': 'ok'}), 200

    try:
        if device_id not in latest_photos_metadata:
            return jsonify({'error': 'No hay foto'}), 404

        photo_info = latest_photos_metadata[device_id]
        filepath = photo_info['filepath']

        if not os.path.exists(filepath):
            return jsonify({'error': 'Archivo no encontrado'}), 404

        client_etag = request.headers.get('If-None-Match')
        server_etag = photo_info.get('etag')

        if config.ENABLE_ETAG and client_etag and server_etag: 
            if client_etag == server_etag:
                return Response(status=304)

        response = send_file(
            filepath,
            mimetype='image/jpeg',
            as_attachment=False,
            download_name=photo_info['filename']
        )

        response.headers['Cache-Control'] = 'no-store, no-cache, must-revalidate, max-age=0'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'

        if server_etag:
            response.headers['ETag'] = server_etag

        return response

    except Exception as e:
        print(f"âŒ Error latest_photo: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/rpi/latest-photo-info/<device_id>', methods=['GET', 'OPTIONS'])
def get_latest_photo_info(device_id):
    if request.method == 'OPTIONS':
        return jsonify({'status': 'ok'}), 200

    try:
        if device_id not in latest_photos_metadata: 
            return jsonify({'error': 'No hay foto'}), 404

        photo_info = latest_photos_metadata[device_id].copy()
        photo_info.pop('filepath', None)

        return jsonify({
            'success': True,
            'device_id': device_id,
            **photo_info
        }), 200

    except Exception as e: 
        print(f"âŒ Error latest_photo_info:  {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/rpi/devices', methods=['GET', 'OPTIONS'])
def list_devices():
    if request.method == 'OPTIONS':
        return jsonify({'status': 'ok'}), 200

    try:
        devices_list = []

        with device_lock:
            for dev_id, info in connected_devices.items():
                device_data = {
                    'device_id': dev_id,
                    'type': info['type'],
                    'ip_local': info['ip_local'],
                    'stream_port': info.get('stream_port', 8889),
                    'streaming_active': info.get('streaming_active', False),
                    'stream_url_local': f"http://{info['ip_local']}:{info.get('stream_port', 8889)}/cam",
                    'stream_url_public': info.get('stream_url_public', None),
                    'tunnel_type': info.get('tunnel_type', 'frp'),
                    'capabilities': info['capabilities'],
                    'connected_at': info['connected_at'],
                    'last_seen_ago': round(time.time() - info.get('last_seen', 0), 1)
                }

                if dev_id in latest_photos_metadata:
                    device_data['has_photo'] = True
                    device_data['last_photo_time'] = latest_photos_metadata[dev_id]['timestamp']
                    device_data['last_photo_url'] = f"/api/rpi/latest-photo/{dev_id}"
                    device_data['last_photo_size_kb'] = latest_photos_metadata[dev_id]['size_kb']
                else:
                    device_data['has_photo'] = False

                devices_list.append(device_data)

        return jsonify({
            'devices': devices_list,
            'total': len(devices_list),
            'timestamp': datetime.now().isoformat()
        }), 200

    except Exception as e:
        print(f"âŒ Error list_devices:  {e}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/rpi/streaming/start/<device_id>', methods=['POST', 'OPTIONS'])
def start_streaming(device_id):
    if request.method == 'OPTIONS':
        return jsonify({'status': 'ok'}), 200

    try:
        if device_id not in connected_devices: 
            return jsonify({'error':  'Dispositivo no encontrado'}), 404

        command = {
            'action': 'start_streaming',
            'params': {},
            'timestamp': time.time()
        }

        with device_lock:
            device_commands[device_id].append(command)
            connected_devices[device_id]['last_seen'] = time.time()

        print(f"ğŸ¬ START STREAMING â†’ {device_id}")

        return jsonify({
            'status': 'command_queued',
            'device_id': device_id,
            'action': 'start_streaming',
            'timestamp': datetime.now().isoformat()
        }), 200

    except Exception as e:
        print(f"âŒ Error start_streaming: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/rpi/streaming/stop/<device_id>', methods=['POST', 'OPTIONS'])
def stop_streaming(device_id):
    if request.method == 'OPTIONS':
        return jsonify({'status': 'ok'}), 200

    try:
        if device_id not in connected_devices: 
            return jsonify({'error':  'Dispositivo no encontrado'}), 404

        command = {
            'action': 'stop_streaming',
            'params': {},
            'timestamp': time.time()
        }

        with device_lock:
            device_commands[device_id].append(command)
            connected_devices[device_id]['last_seen'] = time.time()

        print(f"ğŸ›‘ STOP STREAMING â†’ {device_id}")

        return jsonify({
            'status': 'command_queued',
            'device_id': device_id,
            'action': 'stop_streaming',
            'timestamp': datetime.now().isoformat()
        }), 200

    except Exception as e:
        print(f"âŒ Error stop_streaming: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/rpi/capture/<device_id>', methods=['POST', 'OPTIONS'])
def send_capture_command(device_id):
    if request.method == 'OPTIONS':
        return jsonify({'status': 'ok'}), 200

    try:
        if device_id not in connected_devices: 
            return jsonify({'error':  'Dispositivo no encontrado'}), 404

        data = request.get_json() if request.is_json else {}
        resolution = data.get('resolution', '1920x1080')

        command = {
            'action': 'capture',
            'params': {
                'resolution': resolution,
                'format': 'jpg'
            },
            'timestamp':  time.time()
        }

        with device_lock:
            device_commands[device_id].append(command)
            connected_devices[device_id]['last_seen'] = time.time()

        print(f"ğŸ“¸ CAPTURE â†’ {device_id} ({resolution})")

        return jsonify({
            'status': 'command_queued',
            'device_id': device_id,
            'action': 'capture',
            'timestamp': datetime.now().isoformat()
        }), 200

    except Exception as e:
        print(f"âŒ Error capture: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/rpi/stream-url', methods=['POST', 'OPTIONS'])
def rpi_stream_url():
    if request.method == 'OPTIONS':
        return jsonify({'status': 'ok'}), 200

    try:
        data = request.get_json()
        device_id = data.get('device_id')
        stream_url = data.get('stream_url')
        streaming_active = data.get('streaming_active', True)
        tunnel_type = data.get('tunnel_type', 'frp')

        if not device_id: 
            return jsonify({'error':  'device_id requerido'}), 400

        if device_id not in connected_devices: 
            return jsonify({'error':  'No registrado'}), 404

        with device_lock:
            if stream_url: 
                connected_devices[device_id]['stream_url_public'] = stream_url
                connected_devices[device_id]['tunnel_type'] = tunnel_type
            connected_devices[device_id]['streaming_active'] = streaming_active
            connected_devices[device_id]['last_seen'] = time.time()

        if stream_url:
            print(f"\nğŸŒ URL pÃºblica actualizada: {device_id}")
            print(f"   {stream_url} ({tunnel_type})\n")
        else:
            print(f"ğŸ›‘ Streaming detenido: {device_id}")

        return jsonify({
            'status': 'ok',
            'device_id':  device_id,
            'streaming_active': streaming_active
        }), 200

    except Exception as e:
        print(f"âŒ Error stream_url: {e}")
        return jsonify({'error': str(e)}), 500

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• API REST - DETECCIÃ“N IA CON DUAL MODEL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.route('/health', methods=['GET'])
def health():
    return jsonify({
        'status':  'healthy',
        'models': {
            'legacy': {'loaded': model_legacy_loaded, 'path': config.MODEL_PATH_LEGACY},
            'custom': {'loaded': model_custom_loaded, 'path': config.MODEL_PATH_CUSTOM},
            'default': config.DEFAULT_MODEL
        },
        'connected_devices': len(connected_devices),
        'backend_version': '5.1',
        'upload_method': 'multipart',
        'photos_stored': len(latest_photos_metadata),
        'timestamp': datetime.now().isoformat()
    }), 200

@app.route('/api/predict', methods=['POST', 'OPTIONS'])
def predict():
    """ğŸ†• AnÃ¡lisis IA con selecciÃ³n de modelo"""
    if request.method == 'OPTIONS': 
        return jsonify({'status': 'ok'}), 200

    try:
        # ğŸ†• Detectar modelo solicitado
        model_type = request.form.get('model', config.DEFAULT_MODEL).lower()
        
        if model_type not in ['legacy', 'custom']: 
            return jsonify({'error':  f'Modelo invÃ¡lido: {model_type}.Use "legacy" o "custom"'}), 400

        # Verificar que el modelo estÃ© cargado
        if model_type == 'legacy' and not model_legacy_loaded: 
            return jsonify({'error': 'Modelo Legacy no disponible'}), 503
        
        if model_type == 'custom' and not model_custom_loaded:
            return jsonify({'error': 'Modelo Custom no disponible'}), 503

        if 'image' not in request.files:
            return jsonify({'error': 'No imagen'}), 400

        file = request.files['image']
        if file.filename == '':
            return jsonify({'error': 'Nombre vacÃ­o'}), 400

        use_tta = request.form.get('use_tta', 'true').lower() == 'true'

        filename = secure_filename(file.filename)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"input_{timestamp}_{filename}"
        filepath = os.path.join(config.UPLOAD_FOLDER, filename)
        file.save(filepath)

        print(f"\nğŸ“¥ Procesando IA ({model_type.upper()}): {filename}")

        # Seleccionar modelo
        selected_model = model_legacy if model_type == 'legacy' else model_custom

        # Procesar imagen
        img_original, pred_mask, confidence_map, original_dims = procesar_imagen(
            filepath, use_tta, selected_model, model_type
        )
        
        overlay = crear_overlay(img_original, pred_mask)
        metricas = calcular_metricas(pred_mask, confidence_map)

        # Guardar overlay
        overlay_filename, overlay_filepath = guardar_imagen_temp(overlay, prefix='overlay')

        response_data = {
            'success': True,
            'metricas': metricas,
            'imagen_overlay_url': f'/api/result-image/{overlay_filename}',
            'timestamp': datetime.now().isoformat(),
            'procesamiento':  {
                'model_used': model_type,
                'architecture': config.CUSTOM_ENCODER if model_type == 'custom' else config.LEGACY_ARCHITECTURE,
                'encoder':  config.CUSTOM_ENCODER if model_type == 'custom' else config.LEGACY_ENCODER,
                'has_cbam': config.CUSTOM_USE_CBAM if model_type == 'custom' else False,
                'tta_usado': use_tta,
                'tta_transforms': len(config.TTA_TRANSFORMS) if use_tta else 1,
                'threshold': config.THRESHOLD,
                'target_size': config.TARGET_SIZE,
                'original_dimensions': {
                    'width': original_dims[0],
                    'height': original_dims[1]
                }
            }
        }

        # Borrar imagen de entrada
        os.remove(filepath)

        return jsonify(response_data), 200

    except Exception as e: 
        print(f"âŒ Error predict: {e}")
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/result-image/<filename>', methods=['GET', 'OPTIONS'])
def get_result_image(filename):
    if request.method == 'OPTIONS': 
        return jsonify({'status':  'ok'}), 200

    try:
        if not filename.startswith('overlay_') or '..' in filename:
            return jsonify({'error': 'Nombre de archivo invÃ¡lido'}), 400

        filepath = os.path.join(config.UPLOAD_FOLDER, filename)

        if not os.path.exists(filepath):
            return jsonify({'error': 'Archivo no encontrado'}), 404

        etag = calculate_etag(filepath)

        client_etag = request.headers.get('If-None-Match')
        if config.ENABLE_ETAG and client_etag and etag: 
            if client_etag == etag:
                return Response(status=304)

        response = send_file(
            filepath,
            mimetype='image/jpeg',
            as_attachment=False,
            download_name=filename
        )

        response.headers['Cache-Control'] = 'no-store, no-cache, must-revalidate, max-age=0'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'

        if etag: 
            response.headers['ETag'] = etag

        return response

    except Exception as e:
        print(f"âŒ Error result_image: {e}")
        return jsonify({'error': str(e)}), 500

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FUNCIONES IA (CON SOPORTE DUAL MODEL)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def analizar_orientacion_grieta_mejorada(contour):
    if len(contour) < 5:
        return None, "indefinido"

    try:
        moments = cv2.moments(contour)
        if moments['mu20'] - moments['mu02'] != 0:
            angle_moments = 0.5 * np.arctan2(2 * moments['mu11'],
                                            moments['mu20'] - moments['mu02'])
            angle_moments = np.degrees(angle_moments)
        else:
            angle_moments = None

        [vx, vy, x, y] = cv2.fitLine(contour, cv2.DIST_L2, 0, 0.01, 0.01)
        angle_fit = np.arctan2(vy[0], vx[0]) * 180 / np.pi

        angle = angle_moments if angle_moments is not None else angle_fit
        angle = angle % 180
        if angle < 0:
            angle += 180

        tol = config.ANGLE_TOLERANCE

        if angle < tol or angle > (180 - tol):
            tipo = "horizontal"
        elif abs(angle - 90) < tol:
            tipo = "vertical"
        elif abs(angle - 45) < tol:
            tipo = "diagonal"
        elif abs(angle - 135) < tol:
            tipo = "diagonal"
        else:
            tipo = "irregular"

        return float(angle), tipo

    except:
        return None, "indefinido"

def clasificar_patron_global(contours, mask_binary):
    if len(contours) == 0:
        return {
            'patron': 'sin_grietas',
            'descripcion': 'No se detectaron grietas',
            'causa_probable': 'N/A',
            'severidad_ajuste': 1.0,
            'recomendacion': 'Estructura sin daÃ±os'
        }

    longitudes = np.array([cv2.arcLength(c, False) for c in contours])
    indices_validos = np.where(longitudes >= config.MIN_CRACK_LENGTH)[0]

    if len(indices_validos) == 0:
        return {
            'patron': 'superficial',
            'descripcion':  'Grietas superficiales menores',
            'causa_probable': 'Desgaste superficial',
            'severidad_ajuste': 0.8,
            'recomendacion':  'Monitoreo periÃ³dico'
        }

    top_indices = indices_validos[np.argsort(longitudes[indices_validos])[-config.MAX_GRIETAS_ANALIZAR:]]

    orientaciones = []

    for idx in top_indices:
        angle, tipo = analizar_orientacion_grieta_mejorada(contours[idx])
        if angle is not None:
            orientaciones.append(tipo)

    if not orientaciones:
        return {
            'patron': 'superficial',
            'descripcion': 'Grietas superficiales menores',
            'causa_probable': 'Desgaste superficial',
            'severidad_ajuste': 0.8,
            'recomendacion':  'Monitoreo periÃ³dico'
        }

    tipo_counts = Counter(orientaciones)
    tipo_dominante = tipo_counts.most_common(1)[0][0]
    porcentaje_dominante = tipo_counts[tipo_dominante] / len(orientaciones)
    diversidad = len(tipo_counts)

    if diversidad >= 3 and porcentaje_dominante < 0.5:
        return {
            'patron': 'ramificada_mapa',
            'descripcion': 'PatrÃ³n ramificado - ContracciÃ³n tÃ©rmica',
            'causa_probable': 'Cambios tÃ©rmicos, secado del material',
            'severidad_ajuste': 0.8,
            'recomendacion': 'Monitoreo periÃ³dico'
        }
    elif tipo_dominante == "horizontal" and porcentaje_dominante > 0.55:
        return {
            'patron': 'horizontal',
            'descripcion': 'Grietas predominantemente horizontales',
            'causa_probable': 'FlexiÃ³n estructural, presiÃ³n lateral',
            'severidad_ajuste': 1.1,
            'recomendacion': 'InspecciÃ³n de muros y cimentaciÃ³n'
        }
    elif tipo_dominante == "vertical" and porcentaje_dominante > 0.55:
        return {
            'patron': 'vertical',
            'descripcion': 'Grietas verticales - âš ï¸ CRÃTICO',
            'causa_probable': 'Cargas verticales excesivas, asentamientos',
            'severidad_ajuste': 1.3,
            'recomendacion': 'âš ï¸ InspecciÃ³n estructural URGENTE'
        }
    elif tipo_dominante == "diagonal" and porcentaje_dominante > 0.45:
        return {
            'patron': 'diagonal_escalera',
            'descripcion':  'Grietas diagonales - âš ï¸ MUY CRÃTICO',
            'causa_probable': 'Esfuerzos cortantes, movimiento del terreno',
            'severidad_ajuste': 1.4,
            'recomendacion': 'ğŸ”´ EvaluaciÃ³n estructural CRÃTICA'
        }
    elif diversidad >= 2:
        return {
            'patron': 'mixto',
            'descripcion':  'PatrÃ³n mixto de agrietamiento',
            'causa_probable': 'CombinaciÃ³n de factores',
            'severidad_ajuste': 1.2,
            'recomendacion': 'InspecciÃ³n profesional detallada'
        }
    else:
        return {
            'patron': 'irregular',
            'descripcion': 'PatrÃ³n irregular',
            'causa_probable': 'Causa indeterminada',
            'severidad_ajuste': 1.0,
            'recomendacion': 'InspecciÃ³n profesional'
        }

def analizar_morfologia_detallada(mask, contours):
    mask_binary = (mask > 127).astype(np.uint8)
    patron_info = clasificar_patron_global(contours, mask_binary)

    longitudes = np.array([cv2.arcLength(c, False) for c in contours])
    indices_validos = np.where(longitudes >= config.MIN_CRACK_LENGTH)[0]

    if len(indices_validos) == 0:
        return {
            'patron_general': 'superficial',
            'descripcion_patron': 'Grietas superficiales menores',
            'causa_probable': 'Desgaste superficial',
            'severidad_ajuste':  0.8,
            'recomendacion': 'Monitoreo periÃ³dico',
            'distribucion_orientaciones': {
                "horizontal": 0, "vertical": 0, "diagonal": 0, "irregular": 0
            },
            'num_grietas_analizadas': 0,
            'grietas_principales': []
        }

    top_indices = indices_validos[np.argsort(longitudes[indices_validos])[-config.MAX_GRIETAS_ANALIZAR:]][: :-1]

    grietas_detalle = []
    orientaciones_count = {"horizontal": 0, "vertical":  0, "diagonal": 0, "irregular": 0}

    for rank, idx in enumerate(top_indices, 1):
        contour = contours[idx]
        length = longitudes[idx]
        area = cv2.contourArea(contour)
        angle, tipo = analizar_orientacion_grieta_mejorada(contour)

        width = area / length if length > 0 else 0
        x, y, w, h = cv2.boundingRect(contour)
        aspect_ratio = max(w, h) / min(w, h) if min(w, h) > 0 else 0

        orientaciones_count[tipo] += 1

        grietas_detalle.append({
            'id': rank,
            'longitud_px': round(float(length), 2),
            'area_px': int(area),
            'ancho_promedio_px': round(float(width), 2),
            'angulo_grados': round(angle, 1) if angle else None,
            'orientacion': tipo,
            'aspect_ratio': round(float(aspect_ratio), 2),
            'bbox': {'x': int(x), 'y': int(y), 'width': int(w), 'height': int(h)}
        })

    return {
        'patron_general':  patron_info['patron'],
        'descripcion_patron': patron_info['descripcion'],
        'causa_probable': patron_info['causa_probable'],
        'severidad_ajuste': patron_info['severidad_ajuste'],
        'recomendacion': patron_info.get('recomendacion', 'Monitoreo'),
        'distribucion_orientaciones': orientaciones_count,
        'num_grietas_analizadas': len(grietas_detalle),
        'grietas_principales': grietas_detalle[: 5]
    }

@lru_cache(maxsize=1)
def get_transform():
    return A.Compose([
        A.Resize(config.TARGET_SIZE, config.TARGET_SIZE, interpolation=cv2.INTER_CUBIC),
        A.Normalize(mean=config.MEAN, std=config.STD),
        ToTensorV2()
    ])

def advanced_postprocess(mask):
    mask_np = mask.cpu().numpy() if torch.is_tensor(mask) else mask

    if config.USE_MORPHOLOGY:
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        mask_np = cv2.morphologyEx(mask_np, cv2.MORPH_CLOSE, kernel)
        mask_np = cv2.morphologyEx(mask_np, cv2.MORPH_OPEN, kernel)

    if config.USE_CONNECTED_COMPONENTS:
        mask_binary = (mask_np > 0.5).astype(np.uint8)
        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask_binary, connectivity=8)

        cleaned_mask = np.zeros_like(mask_np)
        for i in range(1, num_labels):
            if stats[i, cv2.CC_STAT_AREA] >= config.MIN_COMPONENT_SIZE:
                cleaned_mask[labels == i] = mask_np[labels == i]

        mask_np = cleaned_mask

    mask_binary = (mask_np > 0.5).astype(bool)
    mask_filled = binary_fill_holes(mask_binary)

    return mask_filled.astype(np.float32)

def predict_with_tta(model, img_tensor, model_type='legacy'):
    """ğŸ†• TTA con soporte para ambos modelos"""
    preds = []

    with torch.no_grad():
        if model_type == 'custom':
            pred, _ = model(img_tensor)
            if isinstance(pred, list):
                pred = pred[-1]
        else:
            pred = model(img_tensor)
        
        pred = torch.sigmoid(pred)
        preds.append(pred)

    if 'hflip' in config.TTA_TRANSFORMS:
        img_hflip = torch.flip(img_tensor, dims=[3])
        with torch.no_grad():
            if model_type == 'custom': 
                pred, _ = model(img_hflip)
                if isinstance(pred, list):
                    pred = pred[-1]
            else: 
                pred = model(img_hflip)
            pred = torch.sigmoid(pred)
            pred = torch.flip(pred, dims=[3])
            preds.append(pred)

    if 'vflip' in config.TTA_TRANSFORMS:
        img_vflip = torch.flip(img_tensor, dims=[2])
        with torch.no_grad():
            if model_type == 'custom':
                pred, _ = model(img_vflip)
                if isinstance(pred, list):
                    pred = pred[-1]
            else:
                pred = model(img_vflip)
            pred = torch.sigmoid(pred)
            pred = torch.flip(pred, dims=[2])
            preds.append(pred)

    if 'rotate90' in config.TTA_TRANSFORMS:
        img_rot90 = torch.rot90(img_tensor, k=1, dims=[2, 3])
        with torch.no_grad():
            if model_type == 'custom': 
                pred, _ = model(img_rot90)
                if isinstance(pred, list):
                    pred = pred[-1]
            else:
                pred = model(img_rot90)
            pred = torch.sigmoid(pred)
            pred = torch.rot90(pred, k=-1, dims=[2, 3])
            preds.append(pred)

    if 'rotate180' in config.TTA_TRANSFORMS:
        img_rot180 = torch.rot90(img_tensor, k=2, dims=[2, 3])
        with torch.no_grad():
            if model_type == 'custom':
                pred, _ = model(img_rot180)
                if isinstance(pred, list):
                    pred = pred[-1]
            else:
                pred = model(img_rot180)
            pred = torch.sigmoid(pred)
            pred = torch.rot90(pred, k=-2, dims=[2, 3])
            preds.append(pred)

    if 'rotate270' in config.TTA_TRANSFORMS:
        img_rot270 = torch.rot90(img_tensor, k=3, dims=[2, 3])
        with torch.no_grad():
            if model_type == 'custom':
                pred, _ = model(img_rot270)
                if isinstance(pred, list):
                    pred = pred[-1]
            else: 
                pred = model(img_rot270)
            pred = torch.sigmoid(pred)
            pred = torch.rot90(pred, k=-3, dims=[2, 3])
            preds.append(pred)

    return torch.stack(preds).mean(dim=0)

def procesar_imagen(image_path, use_tta=True, model=None, model_type='legacy'):
    """ğŸ†• Procesar imagen con modelo seleccionado"""
    if model is None:
        raise RuntimeError("Modelo no especificado")

    img = cv2.imread(str(image_path))
    if img is None:
        raise ValueError("No se pudo cargar la imagen")

    h_orig, w_orig = img.shape[:2]
    original_dimensions = (w_orig, h_orig)

    if max(h_orig, w_orig) > config.MAX_IMAGE_DIMENSION:
        scale = config.MAX_IMAGE_DIMENSION / max(h_orig, w_orig)
        new_w = int(w_orig * scale)
        new_h = int(h_orig * scale)
        img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)

    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    original_size = (img.shape[1], img.shape[0])

    transform = get_transform()
    img_tensor = transform(image=img_rgb)['image'].unsqueeze(0).to(config.DEVICE)

    if use_tta:
        pred = predict_with_tta(model, img_tensor, model_type)
    else:
        with torch.no_grad():
            if model_type == 'custom': 
                pred, _ = model(img_tensor)
                if isinstance(pred, list):
                    pred = pred[-1]
            else:
                pred = model(img_tensor)
            pred = torch.sigmoid(pred)

    confidence_map = pred.cpu().numpy()[0, 0]
    confidence_map = cv2.resize(confidence_map, original_size, interpolation=cv2.INTER_LINEAR)
    confidence_map = advanced_postprocess(torch.from_numpy(confidence_map))
    pred_mask = (confidence_map > config.THRESHOLD).astype(np.uint8) * 255

    return img_rgb, pred_mask, confidence_map, original_dimensions

def crear_overlay(img_original, mask):
    mask_binary = (mask > 127).astype(np.uint8)
    color_mask = np.zeros_like(img_original)

    if config.OVERLAY_COLOR == 'red':
        color_mask[: , :, 0] = mask_binary * 255

    overlay = cv2.addWeighted(img_original, 1.0, color_mask, config.OVERLAY_ALPHA, 0)
    return overlay

def calcular_metricas(mask, confidence_map):
    mask_binary = (mask > 127).astype(np.uint8)

    total_pixeles = mask.size
    pixeles_positivos = mask_binary.sum()
    porcentaje_grietas = (pixeles_positivos / total_pixeles) * 100

    contours, _ = cv2.findContours(mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    num_contours = len(contours)

    if num_contours == 0 or porcentaje_grietas < 0.1:
        return {
            'total_pixeles': int(total_pixeles),
            'pixeles_con_grietas': 0,
            'porcentaje_grietas': 0.0,
            'num_grietas_detectadas': 0,
            'longitud_total_px': 0.0,
            'longitud_promedio_px': 0.0,
            'longitud_maxima_px': 0.0,
            'ancho_promedio_px': 0.0,
            'severidad':  "Sin Grietas",
            'estado': "Sin Grietas Significativas",
            'confianza': 95.0,
            'confidence_max': float(confidence_map.max()),
            'confidence_mean': float(confidence_map.mean()),
            'analisis_morfologico': None
        }

    longitudes = np.array([cv2.arcLength(cnt, False) for cnt in contours])
    total_length = longitudes.sum()
    avg_length = longitudes.mean()
    max_length = longitudes.max()
    avg_width = pixeles_positivos / total_length if total_length > 0 else 0

    morfologia = analizar_morfologia_detallada(mask, contours)

    severidad_ajuste = morfologia['severidad_ajuste']
    porcentaje_ajustado = porcentaje_grietas * severidad_ajuste

    if porcentaje_ajustado < 1: 
        severidad = "Baja"
        estado = "Grietas Menores"
    elif porcentaje_ajustado < 5:
        severidad = "Baja"
        estado = "Grietas Menores"
    elif porcentaje_ajustado < 15:
        severidad = "Media"
        estado = "Grietas Moderadas"
    else:
        severidad = "Alta"
        estado = "Grietas Severas"

    if morfologia['patron_general'] in ['vertical', 'diagonal_escalera']:
        if severidad == "Media":
            severidad = "Media-Alta"
        elif severidad == "Baja" and porcentaje_grietas > 2:
            severidad = "Media"

    confianza = min(95.0, 85.0 + (porcentaje_grietas * 0.5))

    return {
        'total_pixeles': int(total_pixeles),
        'pixeles_con_grietas': int(pixeles_positivos),
        'porcentaje_grietas': round(float(porcentaje_grietas), 2),
        'num_grietas_detectadas': int(num_contours),
        'longitud_total_px': round(float(total_length), 2),
        'longitud_promedio_px': round(float(avg_length), 2),
        'longitud_maxima_px': round(float(max_length), 2),
        'ancho_promedio_px': round(float(avg_width), 2),
        'severidad': severidad,
        'estado': estado,
        'confianza': round(confianza, 1),
        'confidence_max':  float(confidence_map.max()),
        'confidence_mean': float(confidence_map.mean()),
        'analisis_morfologico':  morfologia
    }

def guardar_imagen_temp(img_rgb, prefix='overlay'):
    """Guardar imagen procesada temporalmente"""
    img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')
    filename = f"{prefix}_{timestamp}.jpg"
    filepath = os.path.join(config.UPLOAD_FOLDER, filename)

    cv2.imwrite(filepath, img_bgr, [cv2.IMWRITE_JPEG_QUALITY, config.JPEG_QUALITY])

    return filename, filepath

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INICIALIZACIÃ“N
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "â•" * 100)
print("ğŸš€ CRACKGUARD BACKEND v5.1 - DUAL MODEL SUPPORT")
print("â•" * 100)
print(f"ğŸ“¡ URL: https://crackguard.angelproyect.com")
print(f"âš¡ Device: {config.DEVICE}")
print(f"ğŸ“¤ Upload:  multipart/form-data (3Ã— mÃ¡s rÃ¡pido que base64)")
print(f"ğŸ”’ TÃºnel:  FRP directo (sin Cloudflare)")
print(f"ğŸ§¹ Limpieza automÃ¡tica: Cada {config.CLEANUP_INTERVAL}s")
print(f"ğŸ“¸ Max fotos/device: {config.MAX_PHOTOS_PER_DEVICE}")
print(f"â° RetenciÃ³n fotos: {config.PHOTO_RETENTION_HOURS}h")
print(f"\nğŸ¤– DUAL MODEL SUPPORT:")
print(f"   â€¢ LEGACY: {config.LEGACY_ARCHITECTURE} + {config.LEGACY_ENCODER}")
print(f"   â€¢ CUSTOM: UNet++ CBAM + {config.CUSTOM_ENCODER}")
print(f"   â€¢ DEFAULT: {config.DEFAULT_MODEL.upper()}")
print(f"\nğŸ“ Endpoints:")
print(f"   â€¢ POST /api/rpi/register")
print(f"   â€¢ POST /api/rpi/heartbeat")
print(f"   â€¢ POST /api/rpi/photo (multipart optimizado) ğŸš€")
print(f"   â€¢ GET  /api/rpi/latest-photo/<id> (con ETag)")
print(f"   â€¢ GET  /api/rpi/latest-photo-info/<id>")
print(f"   â€¢ GET  /api/rpi/devices")
print(f"   â€¢ POST /api/rpi/streaming/start/<id>")
print(f"   â€¢ POST /api/rpi/streaming/stop/<id>")
print(f"   â€¢ POST /api/rpi/capture/<id>")
print(f"   â€¢ POST /api/predict (multipart) [ParÃ¡metro:  model=legacy|custom] ğŸ†•")
print(f"   â€¢ GET  /health")
print("â•" * 100 + "\n")

# ğŸ†• Cargar ambos modelos
legacy_ok = cargar_modelo_legacy()
custom_ok = cargar_modelo_custom()

if legacy_ok: 
    print("âœ… Modelo LEGACY cargado")
else:
    print("âš ï¸  Modelo LEGACY no disponible")

if custom_ok:
    print("âœ… Modelo CUSTOM cargado")
else:
    print("âš ï¸  Modelo CUSTOM no disponible")

if not legacy_ok and not custom_ok:
    print("âš ï¸  Backend sin IA - NingÃºn modelo disponible")
else:
    print(f"âœ… Backend con IA - Modelo por defecto: {config.DEFAULT_MODEL.upper()}")

print("\nâœ… Backend corriendo en 0.0.0.0:5000\n")

# Iniciar thread de limpieza de fotos
cleanup_photos_thread = threading.Thread(target=cleanup_old_photos, daemon=True)
cleanup_photos_thread.start()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)